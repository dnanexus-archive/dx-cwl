#!/usr/bin/env python

"""dx-cwl: Compile CWL tools and workflows to DNAnexus applications and workflows"""

# TODO: Extract all type and subcommand-level procedures into their own modules
# with their own unit tests

import argparse
import sys
import os
import subprocess
from pprint import pprint
import json
import tempfile
import glob
import shutil
import yaml
import yaml.constructor
import dxpy
import cwltool.load_tool
import cwltool.workflow
from collections import OrderedDict
import dxpy.api
import csv
import time

# The default instance type for intermediate tools. This requires no provider prefix.
# See <https://wiki.dnanexus.com/api-specification-v1.0.0/instance-types>.
INTERMEDIATE_INSTANCE_TYPE = "mem1_ssd1_x2"

script_dir = os.path.dirname(os.path.realpath(__file__))

def sh(cmd, ignore_error=False):
    try:
        subprocess.check_call(cmd, shell=True)
    except subprocess.CalledProcessError as e:
        if ignore_error:
            return
        else:
            sys.exit(e.returncode)

def shell_suppress(cmd, ignore_error=False):
    out = ""
    try:
        out = subprocess.check_output(cmd, shell=True)
    except subprocess.CalledProcessError as e:
        if ignore_error:
            pass
        else:
            print e.output
            raise
    return out

def makedirs(path):
    try:
        os.makedirs(path)
    except:  # If the directory already exists, continue gracefully
        pass

# Creates a folder in a DNAnexus project and archives an old one if necessary
def dx_mkdir_archive(folder, archive_folder):
    sh("dx mkdir -p '{}'".format(archive_folder))
    try:
        new_folder = "{}/{}-{}".format(archive_folder, folder, time.time())
        sh("dx mkdir -p '{}'".format(new_folder))
        shell_suppress("dx mv '{}' '{}'".format(folder, new_folder))
    except:
        pass
    sh("dx mkdir -p '{}'".format(folder))

def sanitize_path(d):
    if d.startswith("./"):
        return d[2:]
    else:
        return d


def get_image_name(asset_id):
    describe_return_json = subprocess.check_output("dx describe '{0}' --json".format(asset_id), shell=True)
    describe_return_json = json.loads(describe_return_json)
    image_name = describe_return_json['name']
    return image_name


###############################################################
# Procedures to preserve order of config file for readability #
###############################################################

# http://stackoverflow.com/questions/5121931/in-python-how-can-you-load-yaml-mappings-as-ordereddicts
try:
    # included in standard lib from Python 2.7
    from collections import OrderedDict
except ImportError:
    # try importing the backported drop-in replacement
    # it's available on PyPI
    from ordereddict import OrderedDict

class OrderedDictYAMLLoader(yaml.Loader):
    """
    A YAML loader that loads mappings into ordered dictionaries.
    """

    def __init__(self, *args, **kwargs):
        yaml.Loader.__init__(self, *args, **kwargs)

        self.add_constructor(u'tag:yaml.org,2002:map', type(self).construct_yaml_map)
        self.add_constructor(u'tag:yaml.org,2002:omap', type(self).construct_yaml_map)

    def construct_yaml_map(self, node):
        data = OrderedDict()
        yield data
        value = self.construct_mapping(node)
        data.update(value)

    def construct_mapping(self, node, deep=False):
        if isinstance(node, yaml.MappingNode):
            self.flatten_mapping(node)
        else:
            raise yaml.constructor.ConstructorError(None, None,
                'expected a mapping node, but found %s' % node.id, node.start_mark)

        mapping = OrderedDict()
        for key_node, value_node in node.value:
            key = self.construct_object(key_node, deep=deep)
            try:
                hash(key)
            except TypeError, exc:
                raise yaml.constructor.ConstructorError('while constructing a mapping',
                    node.start_mark, 'found unacceptable key (%s)' % exc, key_node.start_mark)
            value = self.construct_object(value_node, deep=deep)
            mapping[key] = value
        return mapping


# https://gist.github.com/miracle2k/3184458
"""Make PyYAML output an OrderedDict.
It will do so fine if you use yaml.dump(), but that generates ugly,
non-standard YAML code.
To use yaml.safe_dump(), you need the following.
"""

def represent_odict(dump, tag, mapping, flow_style=None):
    """Like BaseRepresenter.represent_mapping, but does not issue the sort().
    """
    value = []
    node = yaml.MappingNode(tag, value, flow_style=flow_style)
    if dump.alias_key is not None:
        dump.represented_objects[dump.alias_key] = node
    best_style = True
    if hasattr(mapping, 'items'):
        mapping = mapping.items()
    for item_key, item_value in mapping:
        node_key = dump.represent_data(item_key)
        node_value = dump.represent_data(item_value)
        if not (isinstance(node_key, yaml.ScalarNode) and not node_key.style):
            best_style = False
        if not (isinstance(node_value, yaml.ScalarNode) and not node_value.style):
            best_style = False
        value.append((node_key, node_value))
    if flow_style is None:
        if dump.default_flow_style is not None:
            node.flow_style = dump.default_flow_style
        else:
            node.flow_style = best_style
    return node

##########################################################
# Set up root-level parsing and dnanexus login procedure #
##########################################################

parser = argparse.ArgumentParser()
subparsers = parser.add_subparsers()
parent_parser = argparse.ArgumentParser(add_help=False)

parent_parser.add_argument('--token', help='DNAnexus authentication token', required=True)
parent_parser.add_argument('--project', help='DNAnexus project ID', required=True)
parent_parser.add_argument("--rootdir", help="Root directory to place CWL workflow, tools, and resources", default="/dx-cwl-run")

def dx_login(token, project):
    shell_suppress("dx login --token {} --noprojects".format(args.token))
    shell_suppress("dx select '{}'".format(args.project))

################################################################
# Conventions for extracting names and sources from parsed CWL #
################################################################

# TODO: Ensure conventions used below aren't violated by the reference in some way

def step_name(step):
    return step.id.split("#")[1]

def applet_name(step):
    return os.path.splitext(os.path.basename(step.tool['run']))[0]

def tooldef_fname(step):
    return step.tool['run'][7:]

def workflow_name(workflow_fname):
    # TODO: use splitext here
    return os.path.basename(workflow_fname)[:-4]

def io_name_core(ic):
    return ic.split("#")[1].split('/')[-1]

def io_name(i):
    return io_name_core(i['id'])

def io_source(i):
    return i.split("#")[1]

# TODO: Rename to something more intelligible (i.e. this is generic)
def step_raw_yaml(fname):
    with open(fname) as f:
        step = yaml.load(f, OrderedDictYAMLLoader)
    return step

def step_class(step):
    raw = step_raw_yaml(tooldef_fname(step))
    if 'class' in raw:
        return raw['class']
    raise Exception("No class field found in YAML definition for step: {}".format(tooldef_fname(step)))

def traverse_items(x):
    if isinstance(x, list):
        return [(i['id'], i) for i in x]
    elif isinstance(x, dict):
        return x.items()

# TODO: Come up with generic method to do this
def dx_legal_varname(v):
    return v.replace("-", "_").replace(".", "_").replace("=", "eq")


def transform_tool_varnames(tool):
    import copy
    newtool = copy.deepcopy(tool)
    if tool['inputs'] != []:
        newtool['inputs'] = OrderedDict([(dx_legal_varname(iname), ivalue) for iname, ivalue in traverse_items(tool['inputs'])])

    if tool['outputs'] != []:
        newtool['outputs'] = OrderedDict([(dx_legal_varname(iname), ivalue) for iname, ivalue in traverse_items(tool['outputs'])])

    return newtool

def load_raw_tool(fname):
    with open(fname) as f:
        tool = yaml.load(f, OrderedDictYAMLLoader)
    return transform_tool_varnames(tool)


####################
#  CWL => dx Type  #
####################

def cwl2dx(iovalue):
    if 'type' in iovalue:
        cwltype = iovalue['type']
    else:
        cwltype = iovalue

    optional = False
    typemap = {
        "null": "string",
        "boolean": "boolean",
        "int": "int",
        "long": "int",
        "float": "float",
        "double": "float",
        "string": "string",
        "File": "file",
        "Directory": "hash",
        "stdout": "file",
    }

    optional = False

    optional_indicators = set(["'null'", '"null"', "string", "null"])
    def optional_to_actual(tlist):
        tlist_contains_optional = any([t in tlist for t in optional_indicators])
        if len(tlist) > 1 and tlist_contains_optional:
            # TODO: double-check why this needed to exist
            if len(tlist) == 2 and 'string' in tlist:
                return 'string'
            actual_type = [t for t in tlist if str(t) not in optional_indicators]
            if len(actual_type) == 1:
                return actual_type[0]
            else:
                # TODO: temporary workaround for a particular bcbio step. Should be removable soon.
                #if len(actual_type) == 2 and 'File' in actual_type:
                #    return [t for t in tlist if t != 'File'][0]
                #raise Exception("Actual type list of unexpected size: {}, len={}".format(actual_type, len(actual_type)))
                return actual_type
        else:
            raise Exception("Encountered list as CWL type but no indicators that type is optional. {}".format(tlist))

    def convert_type(cwltype):
        # Secondary files require either an array of files or a hash
        if 'secondaryFiles' in iovalue or isinstance(cwltype, list):
            return "hash"
        elif isinstance(cwltype, str) and cwltype.endswith("[]"):
            basetype = typemap[cwltype[:-2]]
            return "array:{}".format(basetype)
        elif isinstance(cwltype, dict):
            if cwltype['type'] == 'array':
                if isinstance(cwltype['items'], list) or isinstance(cwltype['items'], dict):
                    return "hash"
                else:
                    actual_type = cwltype['items']
                basetype = typemap[actual_type]
                return "array:{}".format(basetype)
            # TODO: Properly handle this type by using DNAnexus 'choices'
            elif cwltype['type'] == 'enum':
                return "string"
            else:
                return "hash"
        else:
            return typemap[cwltype]

    if isinstance(cwltype, list):
        if not any([t in cwltype for t in optional_indicators]):
            return "hash", optional
        else:
            optional = True
            actual_type = optional_to_actual(cwltype)
            return convert_type(actual_type), optional
    else:
        if isinstance(cwltype, str) and cwltype.endswith("?"):
            optional = True
            return convert_type(cwltype[:-1]), optional
        else:
            return convert_type(cwltype), optional


##################################
#  CWL resources => dx Instance  #
##################################
# TODO: use smth like microservice
# TODO: ensure support for other clouds and regions

def prefix_instance_provider(provider, instance_type):
    if provider == 'aws':
        return instance_type
    else:
        return "{}:{}".format(provider, instance_type)

# Obtain list of DNAnexus instances that satisfy desired constraints
def satisfies_constraints(instancedb, cores, ram, disk):
    return [(itype, properties['internalName']) for itype, properties in instancedb.items()
                if ('traits' in properties and
                    properties['traits']['numCores'] >= cores and
                    properties['traits']['totalMemoryMB'] >= ram*1.04858 and
                    properties['traits']['ephemeralStorageGB'] >= disk*0.00104858)]

def choose_ec2_instance(sated):
    with open(os.path.join(script_dir, 'resources', 'meta_ec2.csv')) as f:
        ec2meta_reader = csv.reader(f)
        ec2meta = [row for row in ec2meta_reader]
    instance_price = {row[1]:float(row[22].split(' ')[0][1:]) for row in ec2meta}
    # Lowest cost Linux on demand
    return min(sated, key=lambda x: instance_price[x[1]])

def choose_azure_vm_instance(sated):
    with open(os.path.join(script_dir, 'resources', 'meta_azure_vm.csv')) as f:
        reader = csv.reader(f)
        meta = [row for row in reader]

    prices = { row[0]: float(row[1]) for row in meta }

    return min(sated, key=lambda row: prices[row[1]])

# Heuristic to choose the lowest cost instance that matches
def choose_instance(sated, provider='aws'):
    if provider == 'aws':
        return choose_ec2_instance(sated)
    elif provider == 'azure':
        return choose_azure_vm_instance(sated)
    else:
        raise Exception("Invalid provider: {}".format(provider))

def resource_constraints_to_dx_instance(cores, ram, disk, provider='aws'):
    instancedb = json.loads(open(os.path.join(script_dir, "resources", "meta_dx.json")).read())

    # Filter instance types by provider.
    instancedb = {
        name: meta
        for name, meta in instancedb.iteritems()
        if (provider == 'aws' and ':' not in name) or name.startswith(provider)
    }

    sated = satisfies_constraints(instancedb, cores, ram, disk)
    if len(sated) == 0:
        raise Exception("No instance type matches constraints: # cores {}, ram {} Mib, disk {} Mib".format(cores, ram, disk))
    return choose_instance(sated, provider)[0]




####################
# Tool compilation #
####################

parser_compile_tool = subparsers.add_parser('compile-tool', help="Converts a CWL tool definition to a DNAnexus applet", parents=[parent_parser])
parser_compile_tool.add_argument("tool", help="CWL tool definition file")
parser_compile_tool.add_argument("--assets", help="One or more DNAnexus asset IDs to include in tool.", nargs='+')
parser_compile_tool.add_argument("--bundled", help="One or more DNAnexus bundledDepends file IDs to include in tool.", nargs='+')
parser_compile_tool.add_argument("--extradisk", help="Additional disk space required for instance in mebibytes (2**20)", type=int, default=10000)
parser_compile_tool.add_argument("--instance-provider", help="VM instance provider (default: aws)", choices=["aws", "azure"], default="aws")

def compile_tool(args):
    dx_login(args.token, args.project)
    compile_tool_internal(args.tool, args.assets, args.bundled, extradisk=args.extradisk, provider=args.instance_provider)

# TODO: modify naming convention so this can be used as a module as well as conveniently via commoand line
def compile_tool_internal(tool, assets, bundled, folder='/', extradisk=10000, provider='aws'):
    tool_parsed = cwltool.load_tool.load_tool(tool, cwltool.workflow.defaultMakeTool, strict=False)
    builder = cwltool.builder.Builder()
    builder.job = {}
    builder.requirements = []
    builder.outdir = None
    builder.tmpdir = None
    builder.resources = {}
    # e.g. {'cores': 1, 'ram': 4092, 'outdirSize': 512000, 'tmpdirSize': 512000}
    # See also: http://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement
    req = tool_parsed.evalResources(builder, {})
    # CWL does not explicitly specify input requirements, add these for staging
    inreq, _ = tool_parsed.get_requirement("https://www.dnanexus.com/cwl#InputResourceRequirement")
    insize = int(inreq["indirMin"]) if inreq else 0
    instance = resource_constraints_to_dx_instance(req['cores'], req['ram'],
                                                   insize+req['outdirSize']+req['tmpdirSize']+extradisk,
                                                   provider)
    tool_basedir = os.path.dirname(tool)
    toolname = os.path.basename(os.path.splitext(tool)[0])
    dirname = os.path.join(os.getcwd(), folder[1:], toolname)

    # TODO: split out some of these steps and reuse procedures when possible

    sh("rm -rf '{}'".format(dirname))
    makedirs(dirname+"/resources/home/dnanexus")
    sh("cp '{}' '{}/resources/home/dnanexus/tool.cwl'".format(tool, dirname))
    sh("cp '{}/dx-cwl-applet-code.py' '{}'".format(script_dir, dirname))
    with open(dirname+"/resources/home/dnanexus/output_folder.txt", "w") as f:
        f.write(os.path.join(folder+"-output", "intermediate-results"))
    tool = load_raw_tool(tool)

    images_name_list = []
    if 'hints' in tool:
        for i in tool['hints']:
            if 'dockerPull' in i:
                images_name_list.append(i['dockerPull'])
    if 'label' not in tool:
        tool['label'] = toolname

    def preprocess_tool(tool):
        def obtain_import(k,v):
            if k == '$import':
                full_path = os.path.join(dirname, "resources/home/dnanexus/", sanitize_path(v))
                full_path_basedir = os.path.dirname(full_path)
                sh("mkdir -p '{}'".format(full_path_basedir))
                sh("cp '{}' '{}'".format(os.path.join(tool_basedir, sanitize_path(v)), full_path_basedir))
        if isinstance(tool, dict):
            for k,v in tool.items():
                obtain_import(k,v)
                preprocess_tool(v)
        elif isinstance(tool, list):
            for x in tool:
                preprocess_tool(x)

    preprocess_tool(tool)

    dxapp = {}
    dxapp['name'] = toolname
    dxapp['title'] = toolname
    dxapp['summary'] = tool['label']
    dxapp['description'] = tool['label']
    dxapp['dxapi'] = '1.0.0'
    dxapp['version'] = '0.0.1'
    dxapp['inputSpec'] = []
    if tool['inputs'] != []:
        for iname, ivalue in traverse_items(tool['inputs']):
            dxtype, optional = cwl2dx(ivalue)
            dxinput = {}
            dxinput['name'] = iname
            dxinput['label'] = iname
            dxinput['help'] = ''
            dxinput['class'] = dxtype
            dxinput['optional'] = optional
            dxinput['help'] = ivalue.get('doc', '')
            if 'default' in ivalue:
                if isinstance(ivalue['default'], dict):
                    # Figure out the right thing to do in this case
                    pass
                elif dxtype == 'boolean':
                    dxinput['default'] = ivalue['default']
                    dxinput['optional'] = True
                elif ivalue['default'] is None:
                    pass
                else:
                    dxinput['default'] = ivalue['default']
            dxapp['inputSpec'].append(dxinput)

    dxapp['outputSpec'] = []
    if tool['outputs'] != []:
        for oname, ovalue in traverse_items(tool['outputs']):
            dxoutput = {}
            dxoutput['name'] = oname
            dxtype, optional = cwl2dx(ovalue)
            dxoutput['class'] = dxtype
            dxoutput['optional'] = optional
            dxapp['outputSpec'].append(dxoutput)

    runSpec = {}
    runSpec['interpreter'] = 'python2.7'
    runSpec['file'] = 'dx-cwl-applet-code.py'

    # TODO: determine these policies from some form of CWL metadata or companion file
    runSpec['restartableEntryPoints'] = 'all'
    runSpec['systemRequirements'] = {"*": {"instanceType": instance}}
    runSpec['executionPolicy'] = { "restartOn": {"*": 1 } }
    runSpec['timeoutPolicy'] = {"*": { "hours": 48 }}
    runSpec['distribution'] = "Ubuntu"
    runSpec['release'] = "16.04"
    runSpec['assetDepends'] = []
    runSpec['bundledDepends'] = []
    if assets:
        for x in assets:
            if get_image_name(x) in images_name_list:
                runSpec['assetDepends'].append({"id": x})
        print "Docker images assets used: "
        print runSpec['assetDepends']
    if bundled:
        for x in bundled:
            bundled_name = dxpy.describe(x)['name']
            runSpec['bundledDepends'].append({"name": bundled_name, "id": {"$dnanexus_link": x} })
        print "Bundled depends used: "
        print runSpec['bundledDepends']

            
    if not runSpec['assetDepends']:
        runSpec.pop('assetDepends')

    if not runSpec['bundledDepends']:
        runSpec.pop('bundledDepends')
    dxapp['runSpec'] = runSpec
    dxapp['access'] = {"project": "CONTRIBUTE", "network": ["*"]}
    with open(dirname+"/dxapp.json", "w") as f:
        f.write(json.dumps(dxapp))
    # TODO: perhaps don't call dx build, use API
    appid = json.loads(shell_suppress("dx build -a '{}'".format(dirname)))['id']
    shell_suppress("dx mv '{}' '{}'".format(appid, folder))
    return appid


parser_compile_tool.set_defaults(func=compile_tool)

########################
# Scatter tool builder #
########################

# TODO: Tease out common aspects of both these tool compiles into generic procedures if sensible

def compile_scatter_tool(workflow, sname, step, executable_id, folder='/', provider='aws', assets=None, bundled=None):
    scatter_tool = step.tool['scatter']
    if not isinstance(scatter_tool, list):
        scatter_tool = [scatter_tool]
    scatter_on = [io_name_core(x) for x in scatter_tool]
    applet_name = "scatter-{}".format(sname)
    dirname = os.path.join(os.getcwd(),
                           folder[1:], os.path.splitext(os.path.basename(workflow))[0], applet_name)
    sh("rm -rf '{}'".format(dirname))
    makedirs(dirname+"/resources/home/dnanexus")

    tool = load_raw_tool(tooldef_fname(step))


    # TODO: Improve modularity of this

    code  = "#!/usr/bin/env python\n"
    code += "import os\n"
    code += "import dxpy\n\n"
    code += "@dxpy.entry_point('main')\n"
    code += "def main("

    inputs = []
    optionals = []
    for iname, ivalue in traverse_items(tool['inputs']):
        dxtype, optional = cwl2dx(ivalue)
        if optional:
            optionals.append(iname)
        else:
            inputs.append(iname)

    # Do not use DXExecutable directly as it is a mixin:
    # http://autodoc.dnanexus.com/bindings/python/current/_modules/dxpy/bindings/dxapplet.html#DXExecutable

    code += ", ".join(inputs+["{}=None".format(o) for o in optionals])
    code += "):\n"
    code += "    jobs = []\n"
    # TODO: For dot and cross product in the future, use itertools here
    code += "    for i in " + "zip(" + ", ".join(scatter_on) + ")"  + ":\n"

    # TODO: Move this conditional out of the compiled for loop
    if step_class(step) == 'Workflow':
        code += "        executable = dxpy.DXWorkflow('" + executable_id + "')\n"
    else:
        code += "        executable = dxpy.DXApplet('" + executable_id + "')\n"
    code += "        input_dict = {\n"

    for iname, ivalue in traverse_items(tool['inputs']):
        dxtype, optional = cwl2dx(ivalue)
        if iname in scatter_on:
            ival = "i[" + str(scatter_on.index(iname)) + "]"
        else:
            ival = iname
        code += "        '" + iname + "': " + ival + ",\n"
    code += "        }\n\n"
    code += "        for inp in [" + ",".join(["'{}'".format(x) for x in optionals]) + "]:\n"
    code += "           if not eval(inp):\n"
    code += "               input_dict.pop(inp)\n"
    code += "        jobs.append(executable.run(input_dict, project=dxpy.PROJECT_CONTEXT_ID))\n\n"
    code += "    # Use JBORs/promises and defer to the platform for execution\n"
    code += "    output = {\n"
    for oname, ovalue in traverse_items(tool['outputs']):
        oval = "[j.get_output_ref('{}') for j in jobs]".format(oname)
        code += "        '" + oname + "': " + oval + ",\n"
    code += "    }\n\n"
    code += "    return output\n"
    code += "dxpy.run()\n"



    with open(dirname+"/"+"dx-cwl-scatter-code.py", "w") as f:
        f.write(code)

    dxapp = {}

    dxapp['name'] = applet_name
    dxapp['title'] = applet_name
    dxapp['summary'] = applet_name
    dxapp['description'] = applet_name
    dxapp['dxapi'] = '1.0.0'
    dxapp['version'] = '0.0.1'

    dxapp['inputSpec'] = []

    if 'label' not in tool:
        tool['label'] = applet_name

    if tool['inputs'] != []:
        for iname, ivalue in traverse_items(tool['inputs']):
            dxtype, optional = cwl2dx(ivalue)
            dxinput = {}
            dxinput['name'] = iname
            dxinput['help'] = ''
            dxinput['class'] = dxtype
            dxinput['patterns'] = ['*']
            if iname in scatter_on and dxinput['class'] != 'hash':
                if 'array' in dxinput['class']:
                    dxinput['class'] = 'hash'
                else:
                    dxinput['class'] = "array:"+dxinput['class']
            dxinput['optional'] = optional
            dxapp['inputSpec'].append(dxinput)

    dxapp['outputSpec'] = []
    if tool['outputs'] != []:
        for oname, ovalue in traverse_items(tool['outputs']):
            dxoutput = {}
            dxoutput['name'] = oname
            dxtype, optional = cwl2dx(ovalue)
            dxoutput['class'] = dxtype
            if dxoutput['class'] != 'hash':
                if 'array' in dxoutput['class'] or optional:
                    dxoutput['class'] = 'hash'
                else:
                    dxoutput['class'] = "array:"+dxoutput['class']
            dxoutput['optional'] = optional
            dxapp['outputSpec'].append(dxoutput)

    runSpec = {}
    runSpec['interpreter'] = 'python2.7'
    runSpec['file'] = 'dx-cwl-scatter-code.py'
    runSpec['restartableEntryPoints'] = 'all'
    runSpec['systemRequirements'] = {"*": {"instanceType": "mem1_ssd1_x2"}}
    runSpec['executionPolicy'] = { "restartOn": {"*": 1 } }
    runSpec['timeoutPolicy'] = {"*": { "hours": 48 }}
    runSpec['distribution'] = "Ubuntu"
    runSpec['release'] = "16.04"
    runSpec['assetDepends'] = []
    runSpec['bundledDepends'] = []
    if assets:
        for x in assets:
            runSpec['assetDepends'].append({"id": x})
        print "Docker images assets used: "
        print runSpec['assetDepends']
    if bundled:
        for x in bundled:
            bundled_name = dxpy.describe(x)['name']
            runSpec['bundledDepends'].append({"name": bundled_name, "id": {"$dnanexus_link": x} })
        print "Bundled depends used: "
        print runSpec['bundledDepends']

            
    if not runSpec['assetDepends']:
        runSpec.pop('assetDepends')

    if not runSpec['bundledDepends']:
        runSpec.pop('bundledDepends')
    dxapp['runSpec'] = runSpec
    dxapp['access'] = {"project": "CONTRIBUTE", "network": ["*"]}
    with open(dirname+"/dxapp.json", "w") as f:
        f.write(json.dumps(dxapp))
    appid = json.loads(shell_suppress("dx build -a '{}'".format(dirname)))['id']
    shell_suppress("dx mv '{}' '{}'".format(appid, folder))
    return appid


##################################################################
# Tool builder for Workflows Inputs Containing Inline Javascript #
##################################################################

def compile_inline_js_wf_input(workflow_fname, sname, step, folder, i, sources, provider='aws'):
    sources = [(iname.replace("/", "_"), itype) for iname, itype in sources]
    step_iname = io_name(i)
    applet_name = "inlinejs-{}-{}".format(sname, step_iname)
    dirname = os.path.join(os.getcwd(),
                           folder[1:], os.path.splitext(os.path.basename(workflow_fname))[0], applet_name)
    sh("rm -rf '{}'".format(dirname))
    makedirs(dirname+"/resources/home/dnanexus")
    if 'valueFrom' in i:
        with open("{}/resources/home/dnanexus/expression.txt".format(dirname), "w") as f:
            f.write(i['valueFrom'])

    instance = prefix_instance_provider(provider, INTERMEDIATE_INSTANCE_TYPE)
    tool = load_raw_tool(tooldef_fname(step))


    dxapp = {}

    dxapp['name'] = applet_name
    dxapp['title'] = applet_name
    dxapp['summary'] = applet_name
    dxapp['description'] = applet_name
    dxapp['dxapi'] = '1.0.0'
    dxapp['version'] = '0.0.1'

    code  = "#!/usr/bin/env python\n"
    code += "import os\n"
    code += "import json\n"
    code += "import yaml\n"
    code += "import subprocess\n"
    code += "import dxpy\n\n"
    code += """
def sh(cmd, ignore_error=False):
    try:
        print cmd
        subprocess.check_call(cmd, shell=True)
    except subprocess.CalledProcessError as e:
        if ignore_error:
            return
        else:
            sys.exit(e.returncode)

def shell_suppress(cmd, ignore_error=False):
    out = ""
    try:
        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=True)
    except subprocess.CalledProcessError as e:
        if ignore_error:
            pass
        else:
            print e.output
            raise
    return out

"""
    code += "@dxpy.entry_point('main')\n"
    code += "def main("
    inputs = [iname for iname, itype in sources if not itype[1]]
    optionals = [iname+"=None" for iname, itype in sources if itype[1]]
    code += ", ".join(inputs+optionals)
    code += "):\n"
    code += """
    sh("pip install -U pip wheel setuptools")
    sh("curl https://nodejs.org/dist/v6.11.2/node-v6.11.2-linux-x64.tar.gz | tar xzvf - --strip-components 1 -C /usr/local/ > /dev/null")
    sh("pip install cwltool")
    output = {}
"""
    if 'valueFrom' in i:
        code += """
    import cwltool.expression
    expression = open("expression.txt").read()
"""
    if len(sources) == 1:
        code += "    inputs = %s\n" % (sources[0][0])
    else:
        code += "    inputs = [\n"
        for iname in inputs:
            code += "    " + iname + ",\n"
        code += "    ]\n"

    if 'valueFrom' in i:
        code += """
    output['%s'] = cwltool.expression.do_eval(ex=expression, jobinput=None, outdir=None, tmpdir=None, resources={}, context=inputs, requirements=[{"class":"InlineJavascriptRequirement"}])
""" % (step_iname)
    else:
        code += """
    output['%s'] = inputs
""" % (step_iname)
    code += """
    # Ensure that an optional output (CWL 'null' == JSON None) isn't included in DNAnexus output
    for k,v in output.items():
        if not v:
            del output[k]
    return output
"""

    code += "dxpy.run()\n"
    with open(dirname+"/"+"dx-cwl-inlinejs-wf-input-code.py", "w") as f:
        f.write(code)


    dxapp = {}

    dxapp['name'] = applet_name
    dxapp['title'] = applet_name
    dxapp['summary'] = applet_name
    dxapp['description'] = applet_name
    dxapp['dxapi'] = '1.0.0'
    dxapp['version'] = '0.0.1'

    dxapp['inputSpec'] = []

    print sources
    for iname, itype in sources:
         dxtype, optional = itype
         dxinput = {}
         dxinput['name'] = iname
         dxinput['help'] = ''
         dxinput['class'] = dxtype
         dxinput['patterns'] = ['*']
         dxinput['optional'] = optional
         dxapp['inputSpec'].append(dxinput)

    dxapp['outputSpec'] = []
    if tool['inputs'] != []:
        for iname, ivalue in traverse_items(tool['inputs']):
            if iname == step_iname:
                dxoutput = {}
                dxoutput['name'] = iname
                dxtype, optional = cwl2dx(ivalue)
                dxoutput['class'] = dxtype
                dxoutput['optional'] = optional
                dxapp['outputSpec'].append(dxoutput)


    runSpec = {}
    runSpec['interpreter'] = 'python2.7'
    runSpec['file'] = 'dx-cwl-inlinejs-wf-input-code.py'
    runSpec['restartableEntryPoints'] = 'all'
    runSpec['systemRequirements'] = {"*": {"instanceType": instance}}
    runSpec['executionPolicy'] = { "restartOn": {"*": 1 } }
    runSpec['timeoutPolicy'] = {"*": { "hours": 48 }}
    runSpec['distribution'] = "Ubuntu"
    runSpec['release'] = "16.04"
    dxapp['runSpec'] = runSpec
    dxapp['access'] = {"project": "CONTRIBUTE", "network": ["*"]}
    with open(dirname+"/dxapp.json", "w") as f:
        f.write(json.dumps(dxapp))
    appid = json.loads(shell_suppress("dx build -a '{}'".format(dirname)))['id']
    shell_suppress("dx mv '{}' '{}'".format(appid, folder))
    return appid, applet_name


#####################################
# Workflow postprocess tool builder #
#####################################

def compile_postprocess_tool(workflow, outputs, folder='/', provider='aws'):
    applet_name = "postprocess-outputs"
    dirname = os.path.splitext(workflow)[0]+"/"+applet_name
    sh("rm -rf '{}'".format(dirname))
    makedirs(dirname+"/resources/home/dnanexus")
    with open(dirname+"/resources/home/dnanexus/output_folder.txt", "w") as f:
        f.write(folder+"-output")

    # TODO: Improve modularity of this
    sh("cp '{script_dir}/dx-cwl-postprocess-output-code.py' '{dir}'".format(script_dir=script_dir, dir=dirname))

    instance = prefix_instance_provider(provider, INTERMEDIATE_INSTANCE_TYPE)

    dxapp = {}

    dxapp['name'] = applet_name
    dxapp['title'] = applet_name
    dxapp['summary'] = applet_name
    dxapp['description'] = applet_name
    dxapp['dxapi'] = '1.0.0'
    dxapp['version'] = '0.0.1'

    dxapp['inputSpec'] = []

    for o in outputs:
        dxinput = {}
        dxinput['name'] = o['name']
        dxinput['help'] = ''
        dxinput['class'] = o['class']
        dxinput['optional'] = o['optional']
        dxinput['patterns'] = ['*']
        dxapp['inputSpec'].append(dxinput)

    dxapp['outputSpec'] = dxapp['inputSpec']

    runSpec = {}
    runSpec['interpreter'] = 'python2.7'
    runSpec['file'] = 'dx-cwl-postprocess-output-code.py'
    runSpec['restartableEntryPoints'] = 'all'
    runSpec['systemRequirements'] = {"*": {"instanceType": instance}}
    runSpec['executionPolicy'] = { "restartOn": {"*": 1 } }
    runSpec['timeoutPolicy'] = {"*": { "hours": 48 }}
    runSpec['distribution'] = "Ubuntu"
    runSpec['release'] = "16.04"
    dxapp['runSpec'] = runSpec
    dxapp['access'] = {"project": "CONTRIBUTE", "network": ["*"]}
    with open(dirname+"/dxapp.json", "w") as f:
        f.write(json.dumps(dxapp))
        f.flush()
    appid = json.loads(shell_suppress("dx build -a '{}'".format(dirname)))['id']
    shell_suppress("dx mv '{}' '{}'".format(appid, folder))
    return appid


########################
# Workflow compilation #
########################

parser_compile_workflow = subparsers.add_parser('compile-workflow', help="Converts a CWL workflow to a DNAnexus workflow", parents=[parent_parser])
parser_compile_workflow.add_argument("workflow", help="CWL workflow definition file")
parser_compile_workflow.add_argument("--assets", help="One or more DNAnexus asset IDs to include in tools.", nargs='+')
parser_compile_workflow.add_argument("--bundled", help="One or more DNAnexus bundledDepends file IDs to include in tool.", nargs='+')
parser_compile_workflow.add_argument("--instance-provider", help="VM instance provider (default: aws)", choices=["aws", "azure"], default="aws")

def compile_workflow(args):
    dx_login(args.token, args.project)
    compile_workflow_internal(args.workflow, args.assets, args.bundled, args.token, args.project, args.rootdir, args.instance_provider)


# TODO: dx legal names for workflow params
def compile_workflow_internal(workflow_fname, assets, bundled, token, project, rootdir, provider='aws'):

    # Get 'normalized' CWL parse using cwltool module for use in some areas of compliation
    workflow = cwltool.load_tool.load_tool(workflow_fname, cwltool.workflow.defaultMakeTool, strict=False)


    # Get ordered list of steps for workflow to preserve order in DNAnexus workflow stages
    with open(workflow_fname) as f:
        raw_workflow = yaml.load(f, OrderedDictYAMLLoader)


    wfname = workflow_name(workflow_fname)
    folder = os.path.join(rootdir, wfname)
    dx_mkdir_archive(folder, ".cwl_workflow_archive")

    print "Compiling tools/workflows for each step in the workflow"
    sys.stdout.flush()
    parsed_step = {}
    executable_id = {}

    # TODO: Could be better as a dictionary comprehension
    for step in workflow.steps:
       sname = step_name(step)
       parsed_step[sname] = step

    for sname, svalue in traverse_items(raw_workflow['steps']):
       print "    {}".format(sname)
       sys.stdout.flush()
       step = parsed_step[sname]
       if step_class(step) == 'Workflow':
           w = compile_workflow_internal(tooldef_fname(step), assets, bundled, token, project, rootdir, provider)
           executable_id[sname] = executable_id.get(sname, w)
       else:
           t = compile_tool_internal(tooldef_fname(step), assets, bundled, folder, provider=provider)
           executable_id[sname] = executable_id.get(sname, t)


    print "Compiling CWL workflow to DNAnexus"
    sys.stdout.flush()
    input_params = {}
    input_params['project'] = project
    input_params['name'] = wfname
    input_params['title'] = wfname


    input_params['stages'] = []

    # Add stages in the order they were defined in the YAML (cwltool module does not appear to preserve order)
    scattermap = {}
    for sname, svalue in traverse_items(raw_workflow['steps']):
        step = parsed_step[sname]
        stage = {}
        if 'scatter' in step.tool:
            stage['id'] = 'scatter-'+sname
            stage['name'] = 'scatter-'+sname
            stage['executable'] = compile_scatter_tool(workflow_fname, sname, step, executable_id[sname], folder, provider, args.assets, args.bundled)
            scattermap[sname] = 'scatter-'+sname
        else:
            stage['id'] = step_name(step)
            stage['name'] = step_name(step)
            stage['executable'] = executable_id[stage['name']]
        stage['input'] = {}

        def io_source_dxtype(iosource):
            # Source refers to stage input/output
            if "/" in iosource:
                iostage, ioname = iosource.split("/")
                # TODO: handle the scatter case
                # TODO: dictionarize this for better efficiency
                for source_sname, source_svalue in traverse_items(raw_workflow['steps']):
                    if source_sname == iostage:
                        source_step = parsed_step[source_sname]
                        print tooldef_fname(source_step)
                        source_tool = load_raw_tool(tooldef_fname(source_step))
                        if source_tool['inputs'] != []:
                            for source_iname, source_ivalue in traverse_items(source_tool['inputs']):
                                print source_iname, ioname
                                if source_iname == ioname:
                                    return cwl2dx(source_ivalue)
                        if source_tool['outputs'] != []:
                            for source_iname, source_ivalue in traverse_items(source_tool['outputs']):
                                print source_iname, ioname
                                if source_iname == ioname:
                                    return cwl2dx(source_ivalue)

            # Source refers to workflow input
            else:
                # TODO: dictionarize this for better efficiency
                for i in workflow.tool['inputs']:
                    if io_name(i) == iosource:
                        return cwl2dx(i)

        for i in step.tool['inputs']:
            iname = io_name(i)
            # TODO: modularize this out

            def link_stage_input(input_name, stage_to_modify, input_source):
                if '/' in input_source:
                    link_istage, link_oname = input_source.split('/')
                    if link_istage in scattermap:
                        link_istage = scattermap[link_istage]
                    stage_to_modify['input'][input_name] = dxpy.dxlink({"stage": link_istage, "outputField": link_oname})
                else:
                    stage_to_modify['input'][input_name] = dxpy.dxlink({"workflowInputField": input_source})

            if 'valueFrom' in i or ('source' in i and isinstance(i['source'], list)):
                # Create shim executable for processing inline Javascript
                sources = []
                if 'source' in i:
                    if isinstance(i['source'], list):
                        sources = [(io_source(x), io_source_dxtype(io_source(x))) for x in i['source']]
                    else:
                        sources = [( io_source(i['source']), io_source_dxtype(io_source(i['source'])) )]
                shim_executable, shim_name = compile_inline_js_wf_input(workflow_fname, sname, step, folder, i, sources, provider=provider)
                # Wire workflow so this shim executable is used
                shim_stage = {}

                shim_stage['id'] = shim_name
                shim_stage['name'] = shim_name
                shim_stage['executable'] = shim_executable
                shim_stage['input'] = {}
                for shim_iname, shim_itype in sources:
                    link_stage_input(shim_iname.replace("/", "_"), shim_stage, shim_iname)

                input_params['stages'].append(shim_stage)
                stage['input'][iname] = dxpy.dxlink({"stage": shim_name, "outputField": iname})
            elif 'source' in i:
                isource = io_source(i['source'])
                link_stage_input(iname, stage, isource)
            elif 'default' in i:
                stage['input'][iname] = i['default']

        stage['folder'] = os.path.join(folder+"-output", "intermediate-results")
        sh("dx mkdir -p '{}'".format(stage['folder']))
        input_params['stages'].append(stage)

    input_params['inputs'] = []
    for i in workflow.tool['inputs']:
        dxtype, optional = cwl2dx(i)
        inputSpec = {}
        inputSpec['name'] = io_name(i)
        inputSpec['class'] = dxtype
        inputSpec['optional'] = optional
        input_params['inputs'].append(inputSpec)

    input_params['outputs'] = []
    postprocess_stage = {}
    postprocess_stage['id'] = 'postprocess-outputs'
    postprocess_stage['name'] = postprocess_stage['id']
    postprocess_stage['input'] = {}
    for i in workflow.tool['outputs']:
        outputSpec = {}
        outputSpec['name'] = io_name(i)
        dxtype, optional = cwl2dx(i)
        source_dxtype, source_optional = io_source_dxtype(io_source(i['outputSource']))
        if source_dxtype == 'hash':
            dxtype = 'hash'
        outputSpec['class'] = dxtype
        osource = io_source(i['outputSource'])
        if '/' in osource:
            ostage, oname = osource.split('/')
            if ostage in scattermap:
                ostage = scattermap[ostage]
            outputSpec['outputSource'] = dxpy.dxlink({"stage": ostage, "outputField": oname})
            postprocess_stage['input'][outputSpec['name']] = outputSpec['outputSource']
        else:
            outputSpec['outputSource'] = osource
        outputSpec['optional'] = optional
        input_params['outputs'].append(outputSpec)
    postprocess_stage['executable'] = compile_postprocess_tool(workflow_fname, input_params['outputs'], folder, provider=provider)
    postprocess_stage['folder'] = folder
    input_params['stages'].append(postprocess_stage)

    sh("dx mkdir -p '{}'".format(folder))
    input_params['folder'] = folder
    wfinfo = dxpy.api.workflow_new(input_params, headers={"Authorization": "Bearer {}".format(token)})

    print "Workflow created in {}. ID: {}".format(folder, wfinfo['id'])
    sys.stdout.flush()
    return wfinfo['id']

parser_compile_workflow.set_defaults(func=compile_workflow)


########################
#  Run a CWL workflow  #
########################
parser_run_workflow = subparsers.add_parser('run-workflow', help="Run a CWL workflow on the platform", parents=[parent_parser])
parser_run_workflow.add_argument("workflow", help="Pointer to workflow/applet file or ID on the platform")
parser_run_workflow.add_argument("inputs", help="Pointer to CWL input file on (JSON or YAML) the platform. All files referenced within this file should exist within the project on the platform. Relative paths are supported.")
parser_run_workflow.add_argument("--wait", action='store_true', help="Pointer to CWL input file on (JSON or YAML) the platform. All files referenced within this file should exist within the project on the platform. Relative paths are supported.")

def run_workflow(args):
    dx_login(args.token, args.project)
    run_workflow_internal(args.workflow, args.inputs, args.token, args.project, args.rootdir, args.wait)

def run_workflow_internal(workflow, inputs, token, project, rootdir, wait):
    shell_suppress("dx cd /")
    wid = json.loads(shell_suppress("dx describe '{}' --json".format(workflow)))['id']
    basedir = os.path.dirname(inputs)
    inp = json.loads(shell_suppress("dx cat '{}:{}'".format(project, inputs)))

    #print "Recursively validate that inputs exist on platform and generate DNAnexus inputs"
    def is_file(ivalue):
        return 'class' in ivalue and ivalue['class'] == 'File'

    def is_directory(ivalue):
        return 'class' in ivalue and ivalue['class'] == 'Directory'

    def compile_input_generic(iname, ivalue):
        if isinstance(ivalue, list):
            return [ compile_input_generic(iname, x) for x in ivalue ]
        elif isinstance(ivalue, dict):
            if is_file(ivalue):
                def get_platform_file(ivalue):
                    if 'path' in ivalue:
                        path_or_location = 'path'
                    elif 'location' in ivalue:
                        path_or_location = 'location'
                    else:
                        raise Exception("Neither path nor location provided for CWL input file")
                    # Check if we already have a DNAnexus input ID
                    if ivalue[path_or_location].startswith("file-"):
                        fid = ivalue[path_or_location]
                    # Otherwise look up the item based on the path
                    else:
                        if ivalue[path_or_location].startswith("/") or ivalue[path_or_location].find(":") > 0:
                            dx_path = ivalue[path_or_location]
                        else:
                            dx_path = "/{}/{}".format(basedir, ivalue[path_or_location])
                        fid = json.loads(shell_suppress("dx describe '{}' --json".format(dx_path)))['id']
                    return dxpy.dxlink(fid)
                files = get_platform_file(ivalue)
                if 'secondaryFiles' in ivalue:
                    files = {"primaryFile": files, 'secondaryFiles': compile_input_generic(iname, ivalue['secondaryFiles'])}
                return files
            elif is_directory(ivalue):
                import copy
                directory = copy.deepcopy(ivalue)
                directory['location'] = os.path.join(basedir, sanitize_path(directory['location']))
                return directory
            else:
                return { k : compile_input_generic(k,v) for k,v in ivalue.items() }
        else:
            return ivalue

    dxinputs = { iname : compile_input_generic(iname, ivalue) for iname, ivalue in inp.items() }

    folder = os.path.join(rootdir, "{}-output".format(os.path.basename(workflow)))
    dx_mkdir_archive(folder, ".cwl_workflow_output_archive")
    if wid.startswith("workflow-"):
        dxwf = dxpy.DXWorkflow(wid)
    else:
        dxwf = dxpy.DXApplet(wid)
    analysis = dxwf.run(dxinputs, project=project, folder=folder, headers={"Authorization": "Bearer {}".format(token)})
    if wait:
        analysis.wait_on_done()
        def is_dx_file(ivalue):
            return isinstance(ivalue, dict) and ('$dnanexus_link' in ivalue and ivalue['$dnanexus_link'].startswith("file-") or 'primaryFile' in ivalue)
        def is_dx_directory(ivalue):
            return isinstance(ivalue, dict) and 'class' in ivalue and ivalue['class'] == 'Directory'
        def compile_output_generic(iname, ivalue):
            if isinstance(ivalue, list):
                return [ compile_output_generic(iname, x) for x in ivalue ]
            elif isinstance(ivalue, dict):
                if is_dx_file(ivalue):
                    if 'primaryFile' in ivalue:
                        objid = ivalue['primaryFile']['$dnanexus_link']
                    else:
                        objid = ivalue['$dnanexus_link']
                    file_info = dxpy.api.file_describe(object_id=objid,
                                                       input_params={'project':dxpy.PROJECT_CONTEXT_ID})
                    #sh("mkdir -p {}".format(objid))
                    file_name = file_info['name']
                    dxpy.download_dxfile(objid, file_name)
                    files = {"location": file_name, "path": file_name, "basename": file_name, "class": "File", "size": os.path.getsize(file_name),
                             "checksum": "sha1$"+shell_suppress("sha1sum '{}' | cut -d' ' -f1".format(file_name)).rstrip()}
                    if 'secondaryFiles' in ivalue:
                        files.update({'secondaryFiles': compile_output_generic(iname, ivalue['secondaryFiles'])})
                    return files
                elif is_dx_directory(ivalue):
                    basedir_loc = os.path.dirname(ivalue['location'])
                    sh("mkdir -p '{}'".format(basedir_loc))
                    sh("unset DX_WORKSPACE_ID && dx cd $DX_PROJECT_CONTEXT_ID: && cd '{}' && dx download -rf '{}'".format(basedir_loc, ivalue['location']))
                    return ivalue
                else:
                    return { k : compile_output_generic(k,v) for k,v in ivalue.items() }
            else:
                return ivalue

        # Filter for workflow level outputs -- TODO: remove after change is part of platform proper
        dxoutputs = {iname: ivalue for iname, ivalue in analysis.describe()['output'].items() if "." not in iname}
        cwloutputs = { iname : compile_output_generic(iname, ivalue) for iname, ivalue in dxoutputs.items()  }
        # TODO: return CWL JSON outputs here as well to run conformance testing
        sys.stdout.write(json.dumps(cwloutputs, indent=2))

parser_run_workflow.set_defaults(func=run_workflow)

if __name__ == "__main__":
    args = parser.parse_args()
    args.func(args)
